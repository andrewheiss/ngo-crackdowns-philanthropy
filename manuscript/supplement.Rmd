---
title: "Supplement to Charity During Crackdown: Analyzing the Impact of State Repression of NGOs on Philanthropy"
short-title: Supplement to Charity During Crackdown
author:
- name: Suparna Chaudhry
  affiliation: Christopher Newport University
  email: suparna.chaudhry@cnu.edu
  url: http://www.suparnachaudhry.com/
- name: Andrew Heiss
  affiliation: Brigham Young University
  email: andrew_heiss@byu.edu
  url: https://www.andrewheiss.com/
date: "`r format(Sys.time(), '%B %e, %Y')`"
title-page: false
published: 
git-repo: https://github.com/andrewheiss/ngo-crackdowns-philanthropy
reference-section-title: References
toc: true
appendix: true
---

<!-- supplement.md is generated from supplement.Rmd. Only edit the .Rmd file, *not* the .md file. -->

# Sample

## Recruitment

Experiment participants were recruited via Amazon Mechanical Turk (MTurk), an online crowdsourcing platform that has been extensively used in social science research in recent years. MTurk allows researchers to recruit participants to perform tasks such as filling out surveys and opinion polls, participating in experiments, or coding the content of documents. Researchers advertise their studies as a human intelligence task (HIT) on MTurk, and participants choose only those HITs that interest them, given the promised price and estimated duration of the task. We listed a link to our survey on MTurk on March 22–23, 2018, and offered participants \$0.75 for successfully completing the study. We estimated that the survey would take 5 minutes to complete and paid participants the equivalent of a \$9/hour wage. On average, participants completed the survey in 3 minutes and 20 seconds (see @fig:avg-time). 

![Time spent on experiment](output/figures/avg-time.pdf){#fig:avg-time}

## Sample size

We used power analysis to determine our sample size: with the anticipation of finding a small effect size ($f^2 = 0.02$) at a 0.05 significance level with a power of 0.9, our target sample size was 527, which we then rounded up to 540 in case of error or noncompliance with the survey.

## Demographics and representativeness

MTurk has become an increasingly popular method for fielding survey experiments on cheap semi-representative national samples, though drawing definitive causal inference from convenience samples is not without issues. Previous research has found that MTurk workers tend to be more liberal, younger, and less racially diverse than the general US population [@CliffordJewellWaggoner:2015; @BerniskyHuberLenz:2012]. The results from our experiment generally mirror these findings. @tbl:exp-sample compares demographic characteristics of our sample with national averages from the US Census's Current Population Survey (CPS) [@IPUMS-CPS:2017]. For general demographic information, we use the 2017 Annual Social and Economic Supplement (ASEC) to the CPS. From 2002–2015, the CPS included a Volunteer Supplement every September, so we use 2015 data for data on volunteering and donating to charity. We do not show other respondent demographic details because we do not have good population-level data to compare our sample with. We could theoretically use Pew data to compare political preferences, but Pew collects data on party affiliation, while we collected data about respondents' ideological positions along a conservative–liberal spectrum, which makes the two variables incomparable.

```{r child = "output/tables/tbl-exp-sample.md"}
```

\newpage

As seen in @tbl:exp-sample, our sample is younger, wealthier, and more educated than national averages. This is to be expected, given previous findings about the characteristics of MTurk workers and given that our target population is the portion of Americans willing to give money to charities online, which implies access to technology and disposable income. Our sample self-reports high levels of charitable giving: 82% said they donate at least once a year, while only 49% donate to charity in the previous year nationally. There are several possible explanations for this discrepancy. First, there are differences in the wording of our question and CPS's question—we ask how often respondents typically donate, while CPS asks if respondents have donated (or not) in the past 12 months. Second, respondents were primed and knew that the survey was related to NGOs, and thus might be displaying social desirability bias. Finally, some charitably-oriented respondents may have self-selected into the survey, given that the title listed on MTurk mentioned international nonprofit organizations. We are not overly concerned with these discrepancies, since we aimed our study at those who would be more inclined to donate online.

@tbl:descriptive-stats provides summary statistics for all the variables we collected in our survey.

\stgroup

\renewcommand*{\arraystretch}{1.75}

```{r child = "output/tables/tbl-descriptive-stats.md"}
```

\fingroup


# Experiment

## Preregistration

Prior to launching the survey, we preregistered our hypotheses and research design at the Open Science Framework, and our preregistration protocol is available at [https://osf.io/dx973/](https://osf.io/dx973/). We made two minor deviations from the original preregistration. First, in the *text* of our preregistration, we only specified the non-nested versions of our hypotheses:

1. Donors will give more/be more likely to give to NGOs that face legal crackdowns abroad
2. Donors will give more/be more likely to give to NGOs working on humanitarian issues
3. Donors will give more/be more likely to give to NGOs that do not receive substantial funding from government sources

In our preregistered data analysis *plan*, however, we explained that we will test the interacted versions of the hypotheses (i.e. donors will give more to humanitarian NGOs facing legal crackdowns, etc.). This was an oversight—we inadvertently used overly simple (and incorrect) textual hypotheses, but correctly described the full analysis plan (using the correct, unstated, unstated hypotheses). 

Second, in our preregistration plan, we declared that we would analyze our data with a series of three Bayesian linear regression models with increasing numbers of interaction terms. To find the effect of crackdown within issue area, for instance, we used a $\text{crackdown} × \text{issue}$ term, and to find the effect of crackdown conditioned on issue and funding we used a three-way $\text{crackdown} \times \text{issue} \times \text{funding}$ term. We then planned on reassembling the different coefficient and intercept terms to approximate the mean values in each combination of conditions. To simplify our analyses in this version of the paper, we estimated group means directly with Stan [@stan; @rstan; @r-project]. This approach provides the same results as the regression models with 2- and 3-way interaction terms, but is far simpler to interpret.

## Treatment assignments and CONSORT diagram

To help ensure the quality of the responses we received and filter out workers who try to get through the HIT as quickly as possible without reading the questions [@BerinskyMargolisSances:2014], we included two attention check questions in the experiment. The first check was the second question in the survey (Q1.3) and involved reading several sentences and following instructions to select specific responses. We designed this question to filter out shirking respondents. We included a second, simpler check later in the survey, to ensure that participants were still engaged (Q3.8). Only two participants did not pass this check. 

@fig:consort uses a CONSORT diagram to show the assignment of participants to the eight different experimental conditions. We excluded participants that (1) did not participate in the experiment through MTurk and (2) failed either attention check.

\blandscape

![CONSORT diagram for experiment](output/figures/consort.pdf){#fig:consort}

\elandscape

\newpage


# Priors and models

We use two statistical models for measuring the effect of the crackdown condition on the likelihood of donating and the amount hypothetically donated. Because we measure likelihood of donation with a binary "Likely to donate" vs. "Not likely to donate" variable, we model the proportion of people responding that they would be likely to donate with a binomial distribution. We use a prior distribution of $\text{Beta}(5, 5)$ for $\theta$ to center the probability of responding positively at 50%. In more formal terms, we model this distribution as follows:

$$
\begin{aligned}
n_{\text{group 1, group 2}} &\sim \text{Binomial}(n_{\text{total in group}}, \theta_{\text{group}}) &\text{[likelihood]}\\
\text{Difference} &= n_{\text{group 2}} - n_{\text{group 1}} &\text{[difference in proportions]} \\
n &: \text{Number likely to donate} \\
\\
\theta_{\text{group 1, group 2}} &\sim \text{Beta}(5, 5) &\text{[prior prob. of being likely to donate]}
\end{aligned}
$$

![Prior $\theta$ for binomial models](output/figures/prior-likely.pdf){#fig:prior-likely}

We estimate the mean amount donated in each condition (crackdown vs. no crackdown, humanitarian assistance vs. human rights issues, private vs. government funding) using a *t* distribution. Following @Kruschke:2013, we use an exponential distribution with a rate of 1/29 for the $\nu$ parameter; a normal distribution with the group mean and standard deviation of 10 to capture wider variability in how much respondents might donate for the $\mu$ parameter; and a $\text{Cauchy}(0, 1)$ distribution for the $\sigma$ parameter. In more formal terms, we use the following model and priors:

$$
\begin{aligned}
x_{\text{group 1, group 2}} &\sim \text{Student } t(\nu, \mu, \sigma) &\text{[likelihood]}\\
\text{Difference} &= x_{\text{group 2}} - x_{\text{group 1}} &\text{[difference in means]} \\
x &: \text{Mean amount donated} \\
\\
\nu &\sim \text{Exponential}(1 / 29) &\text{[prior normality]} \\
\mu_{\text{group 1, group 2}} &\sim \mathcal{N}(\bar{x}_{\text{group 1, group 2}}, 10) &\text{[prior donation mean per group]}\\
\sigma_{\text{group 1, group 2}} &\sim \text{Cauchy}(0, 1)&\text{[prior donation sd per group]}
\end{aligned}
$$

![Prior $\nu$, $\mu$, and $\sigma$ for amount models](output/figures/prior-amount.pdf){#fig:prior-amount}


We obtain the posterior distribution of each dependent variable with Markov Chain Monte Carlo (MCMC) sampling and simulate values from the joint posterior distribution of the coefficient parameters. We use Stan [@stan; @rstan; @r-project] to generate 4 MCMC chains with 4,000 iterations in each chain, 2,000 of which are used for warmup. We use the median values from the posterior distributions as point estimates and calculate credible intervals using the 95% highest posterior density.

\newpage

# Survey experiment

## Recruitment and payment

Title of Mechanical Turk HIT

:   "Survey on international nonprofit organizations (~ 5 minutes)"

HIT description

:   "We are conducting an academic survey about international nonprofit organizations and want to know your opinion about them. This survey will take roughly five minutes to complete."

Payment

:   Participants were paid \$0.75 for successfully completing the experiment, commensurate with a \$9/hour wage.

```{r child = "../analysis/text/experiment_blind.md"}
```

\newpage
